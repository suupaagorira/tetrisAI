# サーバー環境変数（技術詳細）

`src/gui/server.ts` で使用される環境変数についての技術的な解説です。これらのパラメータは、AIのトレーニング挙動やパフォーマンスに直接的な影響を与えます。プロジェクトルートの `.env` ファイルで値を設定することで、これらのハイパーパラメータを調整できます。

## シングルプレイヤー・トレーニング設定

一人用のAIエージェントのトレーニングプロセスを制御します。

- `GUI_TRAIN_PARALLEL` (数値, デフォルト: `100`)
  **並列実行ワーカー数**: 同時に実行するトレーニング環境（Web Worker）の数を指定します。値を大きくすると、単位時間あたりにより多くのエピソードを収集でき、学習が高速化します。ただし、CPUコア数やメモリ搭載量に依存し、PCのリソースを大きく消費します。CPUの論理コア数と同程度に設定するのが一般的です。

- `GUI_TRAIN_EPISODES` (数値, デフォルト: `1`)
  **ワーカー毎のエピソード数**: 各ワーカーが担当するゲームプレイ（エピソード）の回数です。この値を増やすと、各ワーカーがより多くの経験を積んでから学習結果を統合するため、学習の安定性が増す可能性がありますが、1バッチあたりの処理時間が長くなります。

- `GUI_TRAIN_MAX_STEPS` (数値, デフォルト: `5000`)
  **最大ステップ数**: 1エピソードあたりの最大行動（テトリミノの落下・回転など）回数。エージェントがゲームオーバーにならずにプレイを続けた場合に、このステップ数で強制的にエピソードを終了します。無限ループを防ぎ、トレーニング時間を管理する目的があります。値を大きくすると、より長期的な戦略を学習する可能性がありますが、トレーニングのサイクルは遅くなります。

- `GUI_TRAIN_GAMMA` (数値, デフォルト: `0.99`)
  **割引率 (Discount Factor)**: 強化学習における時間割引率γ（ガンマ）。将来の報酬を現在の価値に割り引くための係数です。`1`に近いほど、エージェントは遠い未来の報酬を重視する「長期的」な戦略を学習しようとします。逆に`0`に近いと、目先の報酬のみを最大化する「短期的」な戦略になります。`0.99`は一般的な設定値です。

- `GUI_TRAIN_LR` (数値, デフォルト: `0.001`)
  **学習率 (Learning Rate)**: AIの評価関数（重み）を更新する際の更新幅を決定する係数です。値が大きいと学習は速く進みますが、最適解を通り過ぎてしまい学習が不安定になる（発散する）リスクがあります。値が小さいと学習は安定しますが、収束までに時間がかかります。

- `GUI_TRAIN_EXPLORATION` (数値, デフォルト: `0.05`)
  **探索率 (Exploration Rate)**: ε-greedy（イプシロン・グリーディ）戦略における探索の割合ε（イプシロン）です。この確率で、エージェントは現在最善と考える行動（活用）ではなく、ランダムな行動（探索）をとります。これにより、未知のより良い戦略を発見する機会が生まれます。一般的には学習が進むにつれてこの値を小さくする（探索から活用へシフトする）ことが多いですが、この実装では固定値です。

## 対戦トレーニング設定

2体のAIエージェントを競わせる自己対戦（Self-Play）を通じて、より高度な戦略を獲得させるための設定です。

- `GUI_VERSUS_TRAIN_PARALLEL` (数値, デフォルト: `10`)
  **並列実行ワーカー数（対戦）**: 同時に実行する対戦ゲームの数。
  - **値を大きくすると**: より多くの対戦データを同時に収集できるため、学習全体のサイクルが速くなります。例えば、`10`から`20`に増やすと、単純計算で2倍の対戦経験を同じ時間で積むことができます。ただし、CPU負荷も比例して増加します。

- `GUI_VERSUS_TRAIN_EPISODES` (数値, デフォルト: `5`)
  **ワーカー毎のエピソード数（対戦）**: 1つのワーカーが、重みを更新する前に何回の対戦をこなすか。
  - **値を大きくすると**: 1回の重み更新に使われるデータ量が増えるため、学習が安定しやすくなります。例えば、`1`だと毎回の勝敗に大きく影響されますが、`10`にすると10戦分の総合成績で判断するため、まぐれ勝ち・負けの影響が平準化されます。反面、フィードバックのサイクルは遅くなります。

- `GUI_VERSUS_TRAIN_MAX_STEPS` (数値, デフォルト: `2000`)
  **最大ステップ数（対戦）**: 1回の対戦における最大行動回数。
  - **値を大きくすると**: 長期戦の戦略を学習する余地が生まれます。例えば、守備的な戦術や、相手のミスを誘うような高度な駆け引きを学ぶには、ある程度の長さのゲームが必要です。値を小さくしすぎると、速攻型の戦略ばかりが学習される可能性があります。

- `GUI_VERSUS_TRAIN_GAMMA` (数値, デフォルト: `0.99`)
  **割引率（対戦）**: 将来の報酬（相手への攻撃、ライン消去など）をどれだけ重視するか。
  - **値を大きくする（1に近づける）と**: エージェントは、目先の小さな攻撃よりも、将来のTスピンやRENといった大きな攻撃につながるような、長期的な盤面構築を重視するようになります。

- `GUI_VERSUS_TRAIN_LR_P1` / `GUI_VERSUS_TRAIN_LR_P2` (数値, デフォルト: `0.001`)
  **学習率（プレイヤー1/2）**: 各プレイヤーの学習の速さ。
  - **値を大きくすると**: 対戦相手の戦略変化に対して、より速く適応しようとします。例えば、P2の学習率をP1より高く設定すると、P2はP1の戦術を模倣したり、その弱点をつく戦術を素早く見つけたりするかもしれません。しかし、学習が不安定になり、以前の有効な戦略を忘れてしまう「破滅的忘却」のリスクも高まります。

- `GUI_VERSUS_TRAIN_EXPLORATION_P1` / `GUI_VERSUS_TRAIN_EXPLORATION_P2` (数値, デフォルト: `0.02`)
  **探索率（プレイヤー1/2）**: どれだけ「定石」から外れた行動をとるか。
  - **値を大きくすると**: エージェントは、より多くの「奇策」や「新しい戦術」を試すようになります。自己対戦では、両者が同じ戦略に陥って進化が止まる（局所最適解）ことがありますが、探索率を上げることで、その状況を打破するきっかけが生まれる可能性があります。例えば、片方の探索率を少し上げることで、新たな戦術の「揺さぶり」をかけるといった使い方が考えられます。

- `GUI_VERSUS_TRAIN_SEED_BASE` (数値, デフォルト: `1000`)
  **乱数シードのベース値**: 対戦で使われるミノの出現順序などを決定する乱数の初期値。
  - **値を変更すると**: 異なるミノの組み合わせで対戦が開始されます。同じ設定値でシードだけを変えて何度も実行することで、AIの戦略が特定のミノ順序に過剰適合していないか、その堅牢性をテストすることができます。

## サーバー設定

- `PORT` (数値, デフォルト: `5173`)
  **ポート番号**: トレーニング状況などを表示するWeb UIを提供するためのExpressサーバーがリッスンするポート番号です。
